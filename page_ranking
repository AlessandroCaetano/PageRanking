#!/usr/bin/env python3
'''
Implementation of text analysis of issues and commentaries from SoftwarePublico/SoftwarePublico
repository. This script is modularized on the following modules:

    1. Information Extraction: Make a resquest for gitlab api to get all issues
    on SoftwarePublico/SoftwarePublico repo, then all the comments are retrieved.

    2. Generating Valid Structure: After the information retrieval, a dictionary
    is generated with issues numbers and the issues that are linked to it on the comments.
    Then, a networkx directed acyclic graph is generated, to make easier to run
    pageranking on the graph.

    3. The final part is run the page ranking algorithm, it is defined as:

    pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1e-06,
    nstart=None, weight='weight', dangling=None)

    G (graph) – A NetworkX graph. Undirected graphs will be converted to a
    directed graph with two directed edges for each undirected edge.

    alpha (float, optional) – Damping parameter for PageRank, default=0.85.

    personalization (dict, optional) – The “personalization vector” consisting
    of a dictionary with a key for every graph node and nonzero personalization
    value for each node. By default, a uniform distribution is used.

    max_iter (integer, optional) – Maximum number of iterations in power method
    eigenvalue solver.

    tol (float, optional) – Error tolerance used to check convergence in power
    method solver.

    nstart (dictionary, optional) – Starting value of PageRank iteration for each node.

    weight (key, optional) – Edge data key to use as weight. If None weights are set to 1.

    dangling (dict, optional) – The outedges to be assigned to any “dangling” nodes, i.e.,
    nodes without any outedges. The dict key is the node the outedge points to and the dict
    value is the weight of that outedge. By default, dangling nodes are given outedges according
    to the personalization vector (uniform if not specified). This must be selected to result in
    an irreducible transition matrix (see notes under google_matrix). It may be common to have the
    dangling dict to be the same as the personalization dict.

'''

import os
import json
import nltk
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import networkx as nx
import matplotlib.pyplot as plt

#Disabling insecure warning because of spb certificate problems
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

'''
usage ./page_ranking
'''
def get_repo_issues():
    """
    Make HTTP GET request on Software Publico Gitlab and recover all issues
    on SoftwarePublico/SoftwarePublico repo
    """
    issues_json = []
    if not os.path.isfile('./issues.json'):
        for page_number in range(1, 9):
            issues_json += requests.get('https://www.softwarepublico.gov.br/gitlab/api/v3/'
                                        +'projects/30/issues?page=' + str(page_number) +
                                        '&per_page=100&private_token=pUvEiwMsSgJ2JU7DGiaf',
                                        verify=False).json()
        with open('issues.json', 'w') as outfile:
            json.dump(issues_json, outfile)
    else:
        with open('issues.json', 'r') as infile:
            issues_json = json.load(infile)
    return issues_json

def get_issues_comments(issue_id):
    """
    Make HTTP GET request on Software Publico Gitlab and recover all commentaries
    from one issue
    """
    comments_json = []
    if not os.path.exists('./issues_comments'):
        os.makedirs('./issues_comments')

    if not os.path.isfile('./issues_comments/issue_'+str(issue_id)+'_comments.json'):
        comments_json = requests.get('https://www.softwarepublico.gov.br/gitlab/api/v3/'
                                     +'projects/30/issues/'+str(issue_id)+
                                     '/notes?&per_page=100&private_token=pUvEiwMsSgJ2JU7DGiaf',
                                     verify=False).json()
        with open('./issues_comments/issue_'+str(issue_id)+'_comments.json', 'w') as outfile:
            json.dump(comments_json, outfile)
    else:
        with open('./issues_comments/issue_'+str(issue_id)+'_comments.json', 'r') as infile:
            comments_json = json.load(infile)
    return comments_json


def gen_issue_dict():
    """
    Generates a valid dict with issue id as key and issues linked as value
    """
    issues = get_repo_issues()
    issues_dict = {}
    for issue in issues:
        issues_dict[issue['iid']] = []
        comments = get_issues_comments(issue['id'])
        for comment in comments:
            comment_tokens = nltk.word_tokenize(comment['body'])
            for index, word in enumerate(comment_tokens):
                if index < (len(comment_tokens) - 1):
                    if word == '#' and comment_tokens[index+1][:-1].isdigit():
                        if comment_tokens[index+1][-1] == '_':
                            issues_dict[issue['iid']].append(comment_tokens[index+1][:-1])
    return issues_dict

def gen_graph():
    """
    Generates networkx valid DAG based on the issues dictionary
    """
    graph = nx.DiGraph()
    issues_dict = gen_issue_dict()
    for key in issues_dict:
        graph.add_node(key)
    for key, value in issues_dict.items():
        if isinstance(value, list):
            for node in value:
                graph.add_edge(key, node)
        else:
            graph.add_edge(key, value)

    return graph

def run_pagerank():
    """
    Runs pagerank on all SoftwarePublico/SoftwarePublico issues
    """
    graph = gen_graph()
    page_rank = nx.pagerank(graph)
    ranking = []
    for key, value in page_rank.items():
        ranking.append(value)
    print(page_rank)
    print(max(ranking))
    return 0

run_pagerank()
